import os 
from dotenv import load_dotenv
load_dotenv('/media/varun/Data/data science/main_files/.env')


import os 
import google.generativeai as genai 
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

#set the environment variables
os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=os.environ['GOOGLE_API_KEY'])

#load the model 
model=genai.GenerativeModel('gemini-pro')
llm = ChatGoogleGenerativeAI(model="gemini-pro")


from langchain_groq import ChatGroq
os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')

#load the model
llm = ChatGroq(model='gemma2-9b-it')

system = "You are a helpful assistant."
human = "{text}"
prompt = ChatPromptTemplate.from_messages([("system", system), ("human", human)])

chain = prompt | llm








from sqlalchemy import MetaData

metadata_obj = MetaData()


from sqlalchemy import Column, Integer, String, Table, Date, Float

stocks = Table(
    "stocks",
    metadata_obj,
    Column("obs_id", Integer, primary_key=True),
    Column("stock_ticker", String(4), nullable=False),
    Column("price", Float, nullable=False),
    Column("date", Date, nullable=False),
)


from sqlalchemy import create_engine

engine = create_engine("sqlite:///:memory:")
metadata_obj.create_all(engine)


from datetime import datetime

observations = [
    [1, 'ABC', 200, datetime(2023, 1, 1)],
    [2, 'ABC', 208, datetime(2023, 1, 2)],
    [3, 'ABC', 232, datetime(2023, 1, 3)],
    [4, 'ABC', 225, datetime(2023, 1, 4)],
    [5, 'ABC', 226, datetime(2023, 1, 5)],
    [6, 'XYZ', 810, datetime(2023, 1, 1)],
    [7, 'XYZ', 803, datetime(2023, 1, 2)],
    [8, 'XYZ', 798, datetime(2023, 1, 3)],
    [9, 'XYZ', 795, datetime(2023, 1, 4)],
    [10, 'XYZ', 791, datetime(2023, 1, 5)],
]


from sqlalchemy import insert

def insert_obs(obs):
    stmt = insert(stocks).values(
    obs_id=obs[0],
    stock_ticker=obs[1],
    price=obs[2],
    date=obs[3]
    )

    with engine.begin() as conn:
        conn.execute(stmt)


for obs in observations:
    insert_obs(obs)


from langchain.utilities import SQLDatabase
from langchain_experimental.sql import SQLDatabaseChain
db = SQLDatabase(engine)
sql_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)


os.environ['OPENAI_API_KEY'] =os.getenv('OPENAI_API_KEY')
from langchain_community.chat_models import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

model =ChatOpenAI(temperature=0,model = 'gpt-4o-mini')

# model.invoke("what is data science")






from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.agents.agent_types import AgentType

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=SQLDatabaseToolkit(db=db, llm=llm),
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
   
    handle_parsing_errors=True
)


input = ("What is the multiplication of the ratio between stock "
         "prices for 'ABC' and 'XYZ' in January 3rd and the ratio "
         "between the same stock prices in January the 4th?")
print(input)



result = agent_executor.invoke(input)


result








from langchain.agents import load_tools

tools = load_tools(
    ["llm-math"],
    llm=llm
)


from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(memory_key="chat_history")

from langchain.agents import initialize_agent

conversational_agent = initialize_agent(
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    tools=tools,
    llm=llm,
    verbose=True,
    max_iterations=3,
    memory=memory,
)


conversational_agent.invoke("What's the result of an investment of $10,000 growing at 8% annually for 5 years with compound interest?")


print(conversational_agent.agent.llm_chain.prompt.template)


#now testing the memory
conversational_agent.invoke("If we start with $15,000 instead and follow the same 8% annual growth for 5 years with compound interest, how much more would we have compared to the previous scenario?"
)
# here it remembers the above conversation









from langchain import Wikipedia
from langchain.agents.react.base import DocstoreExplorer, Tool

docstore=DocstoreExplorer(Wikipedia())
tools = [
    Tool(
        name="Search",
        func=docstore.search,
        description='search wikipedia'
    ),
    Tool(
        name="Lookup",
        func=docstore.lookup,
        description='lookup a term in wikipedia'
    )
]


docstore_agent = initialize_agent(
    tools,
    llm,
    agent="react-docstore",
    verbose=True,
    max_iterations=3,
    handle_parsing_errors=True
)


docstore_agent.invoke("What were Archimedes' last words?")








from langchain import OpenAI, SerpAPIWrapper
from langchain.agents import initialize_agent, Tool

search = SerpAPIWrapper(serpapi_api_key=os.getenv('serp_api_key'))
tools = [
    Tool(
        name="Intermediate Answer",
        func=search.run,
        description='google search'
    )
]


self_ask_with_search = initialize_agent(tools, llm, agent="self-ask-with-search", verbose=True)


self_ask_with_search.invoke("what was jackie chan doing in the 1950")











import inspect

from getpass import getpass
from langchain import OpenAI
from langchain.chains import LLMChain, ConversationChain
from langchain.chains.conversation.memory import (ConversationBufferMemory,
                                                  ConversationSummaryMemory,
                                                  ConversationBufferWindowMemory,
                                                  ConversationKGMemory)
from langchain.callbacks import get_openai_callback
import tiktoken


conversation = ConversationChain(
    llm=llm,
)


print(conversation.prompt.template)



print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))





#here the entire conversation is the given back to the llm - this is token high as all the
#the conversation is given to the llm 
conversation_buf = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory()
)


conversation_buf("Good morning AI!")



    conversation_buf("My interest here is to explore the potential of integrating Large Language Models with external knowledge")









conversation_sum = ConversationChain(
    llm=llm,
    memory=ConversationSummaryMemory(llm=llm)
)





conversation_bufw = ConversationChain(
    llm=llm,
    memory=ConversationBufferWindowMemory(k=1)
)



