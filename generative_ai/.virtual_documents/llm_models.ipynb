import os 
from dotenv import load_dotenv
load_dotenv('/home/varun/Desktop/main_files/.env')





import os 
import google.generativeai as genai 
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

#set the environment variables
os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=os.environ['GOOGLE_API_KEY'])

#load the model 
model=genai.GenerativeModel('gemini-pro')
llm = ChatGoogleGenerativeAI(model="gemini-pro")
#call the model
# result = llm.invoke("Write a ballad about LangChain")
# print(result.content)





# import requests
# from IPython.display import Image


# from langchain_core.messages import HumanMessage
# from langchain_google_genai import ChatGoogleGenerativeAI

# llm = ChatGoogleGenerativeAI(model="gemini-pro-vision")
# # example
# message = HumanMessage(
#     content=[
#         {
#             "type": "text",
#             "text": "What's in this image?",
#         },  # You can optionally provide text parts
#         {"type": "image_url", "image_url":  "download (1).png" },
#     ]
# )
# llm.invoke([message])






#using human and system message , we need to set the convert system message to human to true
from langchain_core.messages import HumanMessage, SystemMessage

model = ChatGoogleGenerativeAI(model="gemini-pro", convert_system_message_to_human=True)
model(
    [
        SystemMessage(content="Answer only yes or no."),
        HumanMessage(content="Is apple a fruit?"),
    ]
)
for chunk in llm.stream("Write a limerick about LLMs."):
    print(chunk.content)
    print("---")






from langchain_groq import ChatGroq
os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')

#load the model
model_groq = ChatGroq(model='mixtral-8x7b-32768')

system = "You are a helpful assistant."
human = "{text}"
prompt = ChatPromptTemplate.from_messages([("system", system), ("human", human)])

chain = prompt | model_groq
result =chain.invoke({"text": "Explain the importance of low latency LLMs."})
print(result.content)






from langchain_anthropic import AnthropicLLM
from langchain_anthropic import ChatAnthropic

os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')

llm = ChatAnthropic(
    model="claude-3-sonnet-20240229",
    temperature=0,
    max_tokens=1024,
    timeout=None,
    max_retries=2,
    # other params...
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

chain = prompt | llm
chain.invoke(
    {
        "input_language": "English",
        "output_language": "German",
        "input": "I love programming.",
    }
)







os.environ['OPENAI_API_KEY'] =os.getenv('OPENAI_API_KEY')
from langchain_community.chat_models import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

model =ChatOpenAI(temperature=0.2)

chain = prompt|model

chain.invoke(
    {'input_language':'english',
    'output_language':'malayalam',
    'input':'what is your name'}
)






text ="""Low latency Large Language Models (LLMs) are crucial for real-time applications and interactive user experiences. Latency refers to the time it takes for an LLM to process an input and generate a response. Low latency is essential for:

**1. Real-Time Interactions:**

* **Chatbots and virtual assistants:** Providing instant responses to user queries, enabling seamless and natural conversations.
* **Live transcription and translation:** Transcribing speech and translating it into different languages in real time.

**2. Gaming and Virtual Reality:**

* **Interactive game experiences:** Generating content and responding to player actions within milliseconds, creating immersive and engaging gameplay.
* **Virtual reality environments:** Creating realistic and responsive virtual worlds that update and adapt in real time.

**3. Predictive Analytics and Decision-Making:**

* **Fraud detection:** Identifying suspicious transactions in real time and preventing financial losses.
* **Predictive maintenance:** Monitoring equipment and predicting failures before they occur, enabling proactive maintenance and reducing downtime.

**4. Enhanced User Experience:**

* **Faster response times:** Improving the overall user experience by providing information and assistance without delays.
* **Increased engagement:** Low latency LLMs make interactions more responsive and engaging, leading to increased user satisfaction.

**5. Competitive Advantage:**

* **Real-time customer service:** Providing instant support to customers, differentiating businesses from competitors with slow response times.
* **Innovation in new applications:** Enabling the development of novel applications that require real-time language processing, such as voice assistants and automated translations.

**Benefits of Low Latency LLMs:**

* **Improved responsiveness**
* **Enhanced user satisfaction**
* **Increased efficiency and productivity**
* **Competitive advantage**
* **Innovation in new applications**

By minimizing latency, LLMs can unlock the full potential of real-time language processing, empowering businesses and individuals to create more immersive, responsive, and transformative experiences."""





#using configurables
from langchain_core.runnables import ConfigurableField
from langchain_core.prompts import PromptTemplate


template2= """you are an expert in parsing a text information {text} and generating a list of questions based on the information """
prompt_template =PromptTemplate.from_template(template2)


llm= ChatAnthropic(
    model="claude-3-sonnet-20240229",
    temperature=0,
    max_tokens=1024,
    timeout=None,
    max_retries=2,
    # other params...
).configurable_alternatives(
 ConfigurableField(id="llm"),
 default_key="anthropic",
 openai=ChatOpenAI(),
 gemini=ChatGoogleGenerativeAI(model='gemini-pro'),
 mistral = ChatGroq(model='mixtral-8x7b-32768') 
)
chain =prompt_template|llm|StrOutputParser()    
    


gemini = chain.with_config(configurable={'llm':'gemini'}).invoke({'text':text})


gemini








from langchain.text_splitter import RecursiveCharacterTextSplitter
from PyPDF2 import PdfReader
from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

#function to read the pdf document
def pdf_read(pdf_filename):
  text = ""
  pdf_reader = PdfReader(pdf_filename)  # Use the single filename directly
  for page in pdf_reader.pages:
    text += page.extract_text()
  return text

text = pdf_read("2303.11366v4.pdf")

#chunking the text documents to smaller chunks
def get_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_text(text)
    return chunks
chunks = get_chunks(text)  

#embedding the chunks
embeddings = SpacyEmbeddings(model_name="en_core_web_sm")

#create a vector store
def vector_store(chunks):
    vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    vector_store.save_local("faiss_db")
    return vector_store
vector=vector_store(chunks)    
retriever = vector.as_retriever()
# docs =retriever.get_relevant_documents('what is reinforcement learning')

#create RAG chian 
template ="""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

Question: {question} 

Context: {context} """
prompt = PromptTemplate.from_template(template)
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
rag_chain.invoke("what is reinforcement learning")





import fitz  # PyMuPDF
from nltk.tokenize import word_tokenize
import re

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text += page.get_text()
    return text

pdf_path = '/media/varun/Data/resume/res.pdf'

text = extract_text_from_pdf(pdf_path)



text





from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List


class details(BaseModel):
    skills: List[str] = Field(description="The skills mentioned in the text")
    projects: List[str] = Field(description="The projects menstioned in the text")

groq_prompt =f"""you are an expert text analyzer that will analyse the given textt
{text} and identify the skills given in the text and also identify the list of projects 
mentioned in the text.
respond in JSON with 'skills' and 'projects'"""
structured_llm = model_groq.with_structured_output(details,method="json_mode")
result = structured_llm.invoke(groq_prompt.format(text))





result












