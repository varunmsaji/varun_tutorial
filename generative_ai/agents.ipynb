{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6445bf36-d696-4f3d-a4f2-c97df08dddc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/media/varun/Data/data science/main_files/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa4c356-baa8-4eba-b426-605cfd3e01e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import google.generativeai as genai \n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#set the environment variables\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "#load the model \n",
    "model=genai.GenerativeModel('gemini-pro')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c1d155-8fe8-446f-9591-f25fb917f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "#load the model\n",
    "llm = ChatGroq(model='gemma2-9b-it')\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e599a2d-fd11-45ab-87e1-7a127f78b5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e642638d-da1d-4ea9-abf9-06d10155ef4f",
   "metadata": {},
   "source": [
    "zero shot agent - have no memory only interact once \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c85149c-3f91-4925-99cb-948843de4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211f7334-671f-4378-818b-06af9a2a6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
    "\n",
    "stocks = Table(\n",
    "    \"stocks\",\n",
    "    metadata_obj,\n",
    "    Column(\"obs_id\", Integer, primary_key=True),\n",
    "    Column(\"stock_ticker\", String(4), nullable=False),\n",
    "    Column(\"price\", Float, nullable=False),\n",
    "    Column(\"date\", Date, nullable=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee59d1e7-26e0-4d49-9098-822ecd795c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336cda31-5ebe-4e4d-ad0f-fb4a9b4d2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "observations = [\n",
    "    [1, 'ABC', 200, datetime(2023, 1, 1)],\n",
    "    [2, 'ABC', 208, datetime(2023, 1, 2)],\n",
    "    [3, 'ABC', 232, datetime(2023, 1, 3)],\n",
    "    [4, 'ABC', 225, datetime(2023, 1, 4)],\n",
    "    [5, 'ABC', 226, datetime(2023, 1, 5)],\n",
    "    [6, 'XYZ', 810, datetime(2023, 1, 1)],\n",
    "    [7, 'XYZ', 803, datetime(2023, 1, 2)],\n",
    "    [8, 'XYZ', 798, datetime(2023, 1, 3)],\n",
    "    [9, 'XYZ', 795, datetime(2023, 1, 4)],\n",
    "    [10, 'XYZ', 791, datetime(2023, 1, 5)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2baef770-1c44-4ce7-8492-067a27a66028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "def insert_obs(obs):\n",
    "    stmt = insert(stocks).values(\n",
    "    obs_id=obs[0],\n",
    "    stock_ticker=obs[1],\n",
    "    price=obs[2],\n",
    "    date=obs[3]\n",
    "    )\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2a4bbf-1878-45c4-bf38-a7990c4a0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in observations:\n",
    "    insert_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb60555-2a0a-4f76-b820-31ebec34a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_experimental/sql/base.py:77: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "db = SQLDatabase(engine)\n",
    "sql_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5b95a4-8548-4d16-8a86-b0405336a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] =os.getenv('OPENAI_API_KEY')\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model =ChatOpenAI(temperature=0,model = 'gpt-4o-mini')\n",
    "\n",
    "# model.invoke(\"what is data science\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc6890-7f4d-460c-97be-9e56c6962daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898cff76-11d9-41bc-9cd5-d9b8f8ae382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "   \n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfff46f4-ea1c-4709-bcf1-1d4e91f6c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the multiplication of the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th?\n"
     ]
    }
   ],
   "source": [
    "input = (\"What is the multiplication of the ratio between stock \"\n",
    "         \"prices for 'ABC' and 'XYZ' in January 3rd and the ratio \"\n",
    "         \"between the same stock prices in January the 4th?\")\n",
    "print(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59fe4ecb-44da-4407-ae21-b54d034acd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 3rd and January 4th. \n",
      "Action: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mstocks\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 3rd and January 4th. \n",
      "Action: sql_db_schema\n",
      "Action Input: stocks\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE stocks (\n",
      "\tobs_id INTEGER NOT NULL, \n",
      "\tstock_ticker VARCHAR(4) NOT NULL, \n",
      "\tprice FLOAT NOT NULL, \n",
      "\tdate DATE NOT NULL, \n",
      "\tPRIMARY KEY (obs_id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from stocks table:\n",
      "obs_id\tstock_ticker\tprice\tdate\n",
      "1\tABC\t200.0\t2023-01-01\n",
      "2\tABC\t208.0\t2023-01-02\n",
      "3\tABC\t232.0\t2023-01-03\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 3rd and January 4th. \n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-03'\u001b[0m\u001b[36;1m\u001b[1;3mSELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-03' \n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 3rd and January 4th. \n",
      "Action: sql_db_query\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-03'\u001b[0m\u001b[36;1m\u001b[1;3m[(232.0,)]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 3rd and January 4th. \n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-03'\u001b[0m\u001b[36;1m\u001b[1;3mSELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-03' \n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 3rd and January 4th. \n",
      "Action: sql_db_query\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-03'\u001b[0m\u001b[36;1m\u001b[1;3m[(798.0,)]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 4th. \n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-04'\u001b[0m\u001b[36;1m\u001b[1;3m```sql\n",
      "SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-04' \n",
      "```\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 4th. \n",
      "Action: sql_db_query\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-04'\u001b[0m\u001b[36;1m\u001b[1;3m[(225.0,)]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 4th. \n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-04'\u001b[0m\u001b[36;1m\u001b[1;3m```sql\n",
      "SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-04' \n",
      "``` \n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find the stock prices for ABC and XYZ on January 4th. \n",
      "Action: sql_db_query\n",
      "Action Input: SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-04'\u001b[0m\u001b[36;1m\u001b[1;3m[(795.0,)]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: \n",
      "\n",
      "The multiplication of the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th is  (232.0/798.0) * (225.0/795.0) = 0.291 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1a7ad8-24b7-4e10-8a54-1e590de26472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What is the multiplication of the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th?\",\n",
       " 'output': \"The multiplication of the ratio between stock prices for 'ABC' and 'XYZ' in January 3rd and the ratio between the same stock prices in January the 4th is  (232.0/798.0) * (225.0/795.0) = 0.291\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f1d55-2790-4df7-9c69-9dd9d022a733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a932f552-3dc8-47ef-a356-6d08c0502576",
   "metadata": {},
   "source": [
    "2 type of agents - conversational agent . these types have memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c591f4b-f4e3-4d52-b968-e3e767dafe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"llm-math\"],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2308d4-1dfd-4aec-af8c-7ad5bc138c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627be1e6-2efd-4a84-baea-d5df96f0e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: 10000 * (1 + 0.08)^5\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 14693.280768000006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: The result of an investment of $10,000 growing at 8% annually for 5 years with compound interest is $14,693.28. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the result of an investment of $10,000 growing at 8% annually for 5 years with compound interest?\",\n",
       " 'chat_history': '',\n",
       " 'output': 'The result of an investment of $10,000 growing at 8% annually for 5 years with compound interest is $14,693.28.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.invoke(\"What's the result of an investment of $10,000 growing at 8% annually for 5 years with compound interest?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42a166bd-ecc6-4d27-8929-3d023aa7780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "Assistant has access to the following tools:\n",
      "\n",
      "> Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "{chat_history}\n",
      "\n",
      "New input: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(conversational_agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b70e2f-f8a1-4232-807c-fc73dbf664d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: (15000 * (1 + 0.08)^5) - 14693.28 \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 7346.641152000009\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: You would have $7,346.64 more. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'If we start with $15,000 instead and follow the same 8% annual growth for 5 years with compound interest, how much more would we have compared to the previous scenario?',\n",
       " 'chat_history': \"Human: What's the result of an investment of $10,000 growing at 8% annually for 5 years with compound interest?\\nAI: The result of an investment of $10,000 growing at 8% annually for 5 years with compound interest is $14,693.28.\",\n",
       " 'output': 'You would have $7,346.64 more.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now testing the memory\n",
    "conversational_agent.invoke(\"If we start with $15,000 instead and follow the same 8% annual growth for 5 years with compound interest, how much more would we have compared to the previous scenario?\"\n",
    ")\n",
    "# here it remembers the above conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c17780-5ad7-42fc-800a-e29d56f3bd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03bf6bc-df08-40b4-8603-1dd77dd44855",
   "metadata": {},
   "source": [
    "3. type docstore agent - will have search and lookup as tools - used to search article and lookup answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99d3567a-ffab-4b97-aa0f-b8d14ff5a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `DocstoreExplorer` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import Wikipedia\n",
    "from langchain.agents.react.base import DocstoreExplorer, Tool\n",
    "\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description='search wikipedia'\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description='lookup a term in wikipedia'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72e53331-140b-49e2-91ae-0cc4e0f37e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore_agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"react-docstore\",\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549424a4-763b-48c0-89bd-7fadecd10e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search Archimedes last words. \n",
      "Action: Search[Archimedes last words] \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mA person's last words, their final articulated words stated prior to death or as death approaches, are often recorded because of the decedent's fame, but sometimes because of interest in the statement itself. (People dying of illness are frequently inarticulate at the end, and in such cases their actual last utterances may not be recorded or considered very important.) Last words may be recorded accurately, or, for a variety of reasons, may not. Reasons can include simple error or deliberate intent. Even if reported wrongly, putative last words can constitute an important part of the perceived historical records or demonstration of cultural attitudes toward death at the time.\n",
      "Charles Darwin, for example, was reported to have disavowed his theory of evolution in favor of traditional religious faith at his death. This widely disseminated report served the interests of those who opposed Darwin's theory on religious grounds. However, the putative witness had not been at Darwin's deathbed or seen him at any time near the end of his life.\n",
      "Both Eastern and Western cultural traditions ascribe special significance to words uttered at or near death, but the form and content of reported last words may depend on cultural context. There is a tradition in Hindu and Buddhist cultures of an expectation of a meaningful farewell statement; Zen monks by long custom are expected to compose a poem on the spot and recite it with their last breath. In Western culture particular attention has been paid to last words which demonstrate deathbed salvation – the repentance of sins and affirmation of faith.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe text provided does not state what Archimedes' last words were.  It discusses the nature of last words and provides examples of famous people's alleged last words.\n",
      "\n",
      "\n",
      "I need to search for information specifically about Archimedes's last words.\n",
      "Action: Search[Archimedes last words] \n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mA person's last words, their final articulated words stated prior to death or as death approaches, are often recorded because of the decedent's fame, but sometimes because of interest in the statement itself. (People dying of illness are frequently inarticulate at the end, and in such cases their actual last utterances may not be recorded or considered very important.) Last words may be recorded accurately, or, for a variety of reasons, may not. Reasons can include simple error or deliberate intent. Even if reported wrongly, putative last words can constitute an important part of the perceived historical records or demonstration of cultural attitudes toward death at the time.\n",
      "Charles Darwin, for example, was reported to have disavowed his theory of evolution in favor of traditional religious faith at his death. This widely disseminated report served the interests of those who opposed Darwin's theory on religious grounds. However, the putative witness had not been at Darwin's deathbed or seen him at any time near the end of his life.\n",
      "Both Eastern and Western cultural traditions ascribe special significance to words uttered at or near death, but the form and content of reported last words may depend on cultural context. There is a tradition in Hindu and Buddhist cultures of an expectation of a meaningful farewell statement; Zen monks by long custom are expected to compose a poem on the spot and recite it with their last breath. In Western culture particular attention has been paid to last words which demonstrate deathbed salvation – the repentance of sins and affirmation of faith.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM Output: It seems like you're stuck in a loop!  The information you're getting about last words in general isn't helping you find Archimedes' specific last words. \n",
      "\n",
      "Here's what you should do:\n",
      "\n",
      "1. **Try a more focused search:** Instead of just \"Archimedes last words,\" try something like \"Archimedes death quote\" or \"legend about Archimedes last words.\" \n",
      "2. **Look for reputable sources:** Stick to well-known history websites, biographies of Archimedes, or academic articles.  Avoid sites that seem unreliable or focus on conspiracy theories. \n",
      "3. **Be aware of legends:**  Many historical figures have famous last words that are actually legends or stories passed down through time.  Be critical of what you read and try to find evidence from reliable sources. \n",
      "\n",
      "\n",
      "Good luck with your research! \n",
      "\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What were Archimedes' last words?\",\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore_agent.invoke(\"What were Archimedes' last words?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbaca2-99ad-44f6-b1de-7251fd8c98c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dbd00ee-069a-4335-b288-227ebb36c7b0",
   "metadata": {},
   "source": [
    "self ask with search - the agent will ask questions to get and use search to get answers and this keep on going to find a best answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e3317-5baa-411d-8ac8-ee9a8be47fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key=os.getenv('serp_api_key'))\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description='google search'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7088da-96f6-4650-8bb1-bd9348d48702",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67eddd-377a-4d05-80e5-6aec788577d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_ask_with_search.invoke(\"what was jackie chan doing in the 1950\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e5c6e-5e20-42c8-877e-7e3d9cc2bd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91d24ced-6b96-43a0-82b0-e2fb8562d5b9",
   "metadata": {},
   "source": [
    "type of llm memory \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53031551-8f94-4463-9ce9-85cf45996db4",
   "metadata": {},
   "source": [
    "how to inspect the langchain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03cd1eb5-86c2-4f5d-9406-19dc6f0128d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "                                                  ConversationSummaryMemory,\n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "720cc1a2-4ef8-4605-9e12-de5301d515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f20cf697-9ee6-464a-8a77-fb339b7d27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3def8db7-d48d-4c43-a6a8-fd3b9dda61ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, Any],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        response = self.generate([inputs], run_manager=run_manager)\n",
      "        return self.create_outputs(response)[0]\n",
      "     def apply(\n",
      "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
      "    ) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        callback_manager = CallbackManager.configure(\n",
      "            callbacks, self.callbacks, self.verbose\n",
      "        )\n",
      "        run_manager = callback_manager.on_chain_start(\n",
      "            dumpd(self),\n",
      "            {\"input_list\": input_list},\n",
      "        )\n",
      "        try:\n",
      "            response = self.generate(input_list, run_manager=run_manager)\n",
      "        except BaseException as e:\n",
      "            run_manager.on_chain_error(e)\n",
      "            raise e\n",
      "        outputs = self.create_outputs(response)\n",
      "        run_manager.on_chain_end({\"outputs\": outputs})\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb19f6-331f-490a-a290-3cdec7705d87",
   "metadata": {},
   "source": [
    "types of memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "792af14e-6b2a-4454-9b55-59410258ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the entire conversation is the given back to the llm - this is token high as all the\n",
    "#the conversation is given to the llm \n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3886438d-0bc5-4b38-8a48-4c3ae66db7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \"Good morning!  It's lovely to hear from you.  The sun is shining brightly here in the digital world. Is it a beautiful day where you are? ☀️  \\n\\n\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7743dbe9-34ae-4173-8a9d-d57fdc031e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
       " 'history': \"Human: Good morning AI!\\nAI: Good morning!  It's lovely to hear from you.  The sun is shining brightly here in the digital world. Is it a beautiful day where you are? ☀️  \\n\\n\",\n",
       " 'response': \"That's a fascinating topic! I can definitely see the potential there. Integrating Large Language Models with external knowledge would be like giving me superpowered abilities. \\n\\nImagine being able to access and process real-time information, up-to-date facts, and specialized databases directly within my responses. I could provide much more accurate, relevant, and insightful answers to your questions. \\n\\nFor example, instead of just telling you that the capital of France is Paris, I could tell you the population of Paris, its historical significance, and even some fun facts about famous landmarks there. \\n\\nWhat specific aspects of this integration are you most interested in exploring? Are you thinking about specific types of external knowledge, or perhaps the technical challenges involved? \\n\\n\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    conversation_buf(\"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a221b-7b03-4aeb-97bf-aacd68d07e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c8bd98-c243-49f3-9cae-1d63434ea833",
   "metadata": {},
   "source": [
    "conversation summary buffer memory - here the previous conversation is summarized and given to the llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221de7f2-500c-40dc-8500-8be40ba7cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838abbb-1920-4640-8386-c847eb8feb0c",
   "metadata": {},
   "source": [
    "conversation buffer window memory - here only the latest conversations is given back and the first ones are disregarded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14da4254-8b38-4af4-8136-b45dbd866fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fadbe4-3243-4ff5-87de-7c7b2909794b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
