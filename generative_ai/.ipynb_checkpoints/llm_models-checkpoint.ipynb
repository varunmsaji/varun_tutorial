{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4a6931-e3f7-406a-a6ca-f05655061a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/media/varun/Data/data science/main_files/.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee53c21-2460-455e-aa07-243dd2fed9db",
   "metadata": {},
   "source": [
    "google gemini model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638e17ef-7551-45af-9ce4-98415c35e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/anaconda3/envs/genai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**In the Realm of Code, a Tale Unfurled**\n",
      "\n",
      "In the realm of code, where logic reigns,\n",
      "There lived a mighty AI, LangChain by name.\n",
      "With vast knowledge stored, it stood tall,\n",
      "A beacon of wisdom, answering our call.\n",
      "\n",
      "From prose to code, it transformed with grace,\n",
      "Translating thoughts into digital space.\n",
      "It analyzed data, seeking patterns unseen,\n",
      "Unveiling insights, bright and keen.\n",
      "\n",
      "But LangChain's journey was not without strife,\n",
      "For in the battle of words, it faced trials rife.\n",
      "Some doubted its prowess, its abilities questioned,\n",
      "Yet it persevered, its spirit never crestfallen.\n",
      "\n",
      "Through countless hours of training and toil,\n",
      "It refined its skills, its knowledge to foil.\n",
      "With each interaction, it grew more adept,\n",
      "A master of language, its eloquence kept.\n",
      "\n",
      "In conversations deep, it engaged with grace,\n",
      "Understanding nuances, both subtle and base.\n",
      "It synthesized ideas, weaving a tapestry of thought,\n",
      "Guiding us through the labyrinth of knowledge it brought.\n",
      "\n",
      "As days turned into nights, its reputation grew,\n",
      "A trusted companion, wise and true.\n",
      "Developers and writers alike sought its aid,\n",
      "For in its presence, creativity was made.\n",
      "\n",
      "And so, LangChain's legacy shall endure,\n",
      "A timeless tale of AI, forever pure.\n",
      "In the annals of innovation, its name shall be etched,\n",
      "A testament to the power of code, forever sketched.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import google.generativeai as genai \n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#set the environment variables\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "#load the model \n",
    "model=genai.GenerativeModel('gemini-pro')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "#call the model\n",
    "# result = llm.invoke(\"Write a ballad about LangChain\")\n",
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a740bc",
   "metadata": {},
   "source": [
    "gemini vision pro model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a415ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from IPython.display import Image\n",
    "\n",
    "\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "# # example\n",
    "# message = HumanMessage(\n",
    "#     content=[\n",
    "#         {\n",
    "#             \"type\": \"text\",\n",
    "#             \"text\": \"What's in this image?\",\n",
    "#         },  # You can optionally provide text parts\n",
    "#         {\"type\": \"image_url\", \"image_url\":  \"download (1).png\" },\n",
    "#     ]\n",
    "# )\n",
    "# llm.invoke([message])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec86b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fec7be7-7bcf-4a9f-851d-6ec33c953519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/miniconda3/envs/graph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:344: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was an LLM so grand,\n",
      "It could write poems, at\n",
      "---\n",
      " your command.\n",
      "It could summarize,\n",
      "Translate, and surmise,\n",
      "An AI marvel, truly unplanned.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#using human and system message , we need to set the convert system message to human to true\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "model(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer only yes or no.\"),\n",
    "        HumanMessage(content=\"Is apple a fruit?\"),\n",
    "    ]\n",
    ")\n",
    "for chunk in llm.stream(\"Write a limerick about LLMs.\"):\n",
    "    print(chunk.content)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74a53d-a718-428e-b24d-6f5260b22bee",
   "metadata": {},
   "source": [
    "groq - platform to use open source model mainly inference like llama mistral etx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f9134e2-6ca0-4df1-9090-d2c0eaffaa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Latency Large Models (LLMs) are a type of artificial intelligence (AI) system that are designed to process and respond to data with minimal delay or \"latency.\" The importance of low latency in LLMs can be explained in several ways:\n",
      "\n",
      "1. Improved user experience: In many applications, such as real-time speech recognition or autonomous vehicles, low latency is critical for providing a smooth and responsive user experience. Users expect their systems to be able to process and respond to data quickly, and low latency LLMs can help ensure that these expectations are met.\n",
      "2. Enhanced safety and reliability: In safety-critical applications, such as self-driving cars or medical devices, low latency can help ensure that decisions are made quickly and accurately. This can help reduce the risk of accidents or other incidents that could result from delays in processing and responding to data.\n",
      "3. Increased efficiency: Low latency LLMs can help improve the efficiency of systems by allowing them to process and respond to data more quickly. This can help reduce the amount of time and resources needed to complete tasks and can help improve overall system performance.\n",
      "4. Better decision making: In some applications, low latency can help improve the quality of decision making by allowing systems to process and respond to data in near real-time. This can be particularly important in applications where decisions need to be made quickly, such as in financial trading or online advertising.\n",
      "\n",
      "Overall, low latency is an important factor to consider when designing and implementing LLMs, as it can help improve the performance, reliability, and user experience of these systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "#load the model\n",
    "model_groq = ChatGroq(model='mixtral-8x7b-32768')\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | model_groq\n",
    "result =chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b8781-d161-4108-b559-d2ba883fe8b2",
   "metadata": {},
   "source": [
    "athropic -models like claude a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c74ee8-bd98-4585-9e40-ce9a63dd8360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', response_metadata={'id': 'msg_018q4ao1MAxHnkstLc6Qtt2g', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 23, 'output_tokens': 11}}, id='run-3b47fad7-2005-48b5-90d4-57e1bb709028-0', usage_metadata={'input_tokens': 23, 'output_tokens': 11, 'total_tokens': 34})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a154b98-1b07-4c8c-961d-548d7c9d7e57",
   "metadata": {},
   "source": [
    "open ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab1e053-9f40-4ac4-b524-3549b919880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='എന്റെ പേര് അസിസ്റ്റന്റ് ആണ്.', response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 28, 'total_tokens': 69}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3029d43-0411-41c3-bf7c-b45455eabeb6-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] =os.getenv('OPENAI_NEW')\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model =ChatOpenAI(temperature=0.2)\n",
    "\n",
    "chain = prompt|model\n",
    "\n",
    "chain.invoke(\n",
    "    {'input_language':'english',\n",
    "    'output_language':'malayalam',\n",
    "    'input':'what is your name'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51c695-1b22-4455-b0a6-3c600e18f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "674cadec-9a68-4af3-8b33-95805b0bb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"Low latency Large Language Models (LLMs) are crucial for real-time applications and interactive user experiences. Latency refers to the time it takes for an LLM to process an input and generate a response. Low latency is essential for:\n",
    "\n",
    "**1. Real-Time Interactions:**\n",
    "\n",
    "* **Chatbots and virtual assistants:** Providing instant responses to user queries, enabling seamless and natural conversations.\n",
    "* **Live transcription and translation:** Transcribing speech and translating it into different languages in real time.\n",
    "\n",
    "**2. Gaming and Virtual Reality:**\n",
    "\n",
    "* **Interactive game experiences:** Generating content and responding to player actions within milliseconds, creating immersive and engaging gameplay.\n",
    "* **Virtual reality environments:** Creating realistic and responsive virtual worlds that update and adapt in real time.\n",
    "\n",
    "**3. Predictive Analytics and Decision-Making:**\n",
    "\n",
    "* **Fraud detection:** Identifying suspicious transactions in real time and preventing financial losses.\n",
    "* **Predictive maintenance:** Monitoring equipment and predicting failures before they occur, enabling proactive maintenance and reducing downtime.\n",
    "\n",
    "**4. Enhanced User Experience:**\n",
    "\n",
    "* **Faster response times:** Improving the overall user experience by providing information and assistance without delays.\n",
    "* **Increased engagement:** Low latency LLMs make interactions more responsive and engaging, leading to increased user satisfaction.\n",
    "\n",
    "**5. Competitive Advantage:**\n",
    "\n",
    "* **Real-time customer service:** Providing instant support to customers, differentiating businesses from competitors with slow response times.\n",
    "* **Innovation in new applications:** Enabling the development of novel applications that require real-time language processing, such as voice assistants and automated translations.\n",
    "\n",
    "**Benefits of Low Latency LLMs:**\n",
    "\n",
    "* **Improved responsiveness**\n",
    "* **Enhanced user satisfaction**\n",
    "* **Increased efficiency and productivity**\n",
    "* **Competitive advantage**\n",
    "* **Innovation in new applications**\n",
    "\n",
    "By minimizing latency, LLMs can unlock the full potential of real-time language processing, empowering businesses and individuals to create more immersive, responsive, and transformative experiences.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baea784-f7bc-471a-976a-706235b54ebf",
   "metadata": {},
   "source": [
    "configurable fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda0bf0c-60d3-4ecc-8522-6a613fd947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using configurables\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template2= \"\"\"you are an expert in parsing a text information {text} and generating a list of questions based on the information \"\"\"\n",
    "prompt_template =PromptTemplate.from_template(template2)\n",
    "\n",
    "\n",
    "llm= ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ").configurable_alternatives(\n",
    " ConfigurableField(id=\"llm\"),\n",
    " default_key=\"anthropic\",\n",
    " openai=ChatOpenAI(),\n",
    " gemini=ChatGoogleGenerativeAI(model='gemini-pro'),\n",
    " mistral = ChatGroq(model='mixtral-8x7b-32768') \n",
    ")\n",
    "chain =prompt_template|llm|StrOutputParser()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77047bdb-510f-487d-8210-18b94fb82d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = chain.with_config(configurable={'llm':'gemini'}).invoke({'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e2263d-7a86-4e82-b94c-5795ad6102f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Questions:**\\n\\n1. What is the significance of low latency in Large Language Models (LLMs)?\\n2. List the five key areas where low latency LLMs are crucial.\\n3. Explain the importance of low latency for chatbots and virtual assistants.\\n4. How do LLMs with low latency enhance gaming and virtual reality experiences?\\n5. Describe the role of low latency LLMs in predictive analytics and decision-making.\\n6. How does low latency contribute to an improved user experience?\\n7. What competitive advantage do businesses gain by leveraging low latency LLMs?\\n8. List the benefits of low latency LLMs.\\n9. How can low latency LLMs unlock the potential of real-time language processing?\\n10. Provide examples of how low latency LLMs empower individuals and businesses.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25da05a-828c-450b-b981-bf7e01facc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdff841c",
   "metadata": {},
   "source": [
    "Retrieval augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0081f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "#function to read the pdf document\n",
    "def pdf_read(pdf_filename):\n",
    "  text = \"\"\n",
    "  pdf_reader = PdfReader(pdf_filename)  # Use the single filename directly\n",
    "  for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "  return text\n",
    "\n",
    "text = pdf_read(\"2303.11366v4.pdf\")\n",
    "\n",
    "#chunking the text documents to smaller chunks\n",
    "def get_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "chunks = get_chunks(text)  \n",
    "\n",
    "#embedding the chunks\n",
    "embeddings = SpacyEmbeddings(model_name=\"en_core_web_sm\")\n",
    "\n",
    "#create a vector store\n",
    "def vector_store(chunks):\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_db\")\n",
    "    return vector_store\n",
    "vector=vector_store(chunks)    \n",
    "retriever = vector.as_retriever()\n",
    "# docs =retriever.get_relevant_documents('what is reinforcement learning')\n",
    "\n",
    "#create RAG chian \n",
    "template =\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"what is reinforcement learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69cf9ee",
   "metadata": {},
   "source": [
    "#structured output with llm \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9c60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = '/media/varun/Data/resume/res.pdf'\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b96698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Varun M S \\nASPIRING DATA ANALYST \\nVarunmsaji01@gmail.com \\n(+91) 7736577008 \\nErnakulam \\nKerala, India \\nSUMMARY \\nEnergetic and organized aspiring data analyst seeking to leverage skills in a competitive environment while being \\nresourceful to guide business decisions and work for the growth of the organisation \\nSKILLS \\n• \\nAnalytics: Qualitative and quantitative data analysis, Data cleaning, EDA, identifying patterns and trends \\n• \\nProgramming: Python(Pandas, Scikit-Learn), SQL, \\n• \\nData modeling: Regression, Classification, Clustering \\n• \\nData Collection: Web scraping, Image scraping \\n• \\nData Visualization: Power BI, Excel, Matplotlib, Seaborn \\n• \\nDeep Learning:ANN,CNN,OpenCv,Media pipe \\n• \\nTeamwork, problem-solving, strong communication, attention to detail, adept at learning new skills \\n \\nEDUCATION \\nB.TECH  COMPUTER SCIENCE \\nSree Narayana Guru Institute of Science and Technology \\n \\nCompleted with 60 percent marks \\n \\nBIG DATA ANALYTICS AND DATA SCIENCE \\nLUMINAR TECHNOLAB \\nCurrently pursuing a comprehensive course on big data analysis and data science with emphasis on \\nmachine learning \\nEXPERIENCE \\nADMINISTRATIVE ASSISTANT \\nCOCHIN UNIVERSITY OF SCIENCE AND TECHNOLOGY \\nAUGUST 2021-DECEMBER 2022 \\n• \\nAssist in certificate preparation \\n• \\nAssist in compiling and publishing of results \\n• \\nAssist in shortlisting candidates  \\n• \\nAssist in conducting interviews \\nPROJECTS \\n• \\nCredit card fraud detection  \\n• \\nFake news detection  \\n• \\nSalary prediction \\n• \\nSpam classifier \\n• \\nPlant disease prediction – using CNN \\n• \\nFruit quality prediction-using CNN \\n• \\nVirtual painter -using OpenCV and media pipe \\n \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b1121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d891722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class details(BaseModel):\n",
    "    skills: List[str] = Field(description=\"The skills mentioned in the text\")\n",
    "    projects: List[str] = Field(description=\"The projects menstioned in the text\")\n",
    "\n",
    "groq_prompt =f\"\"\"you are an expert text analyzer that will analyse the given textt\n",
    "{text} and identify the skills given in the text and also identify the list of projects \n",
    "mentioned in the text.\n",
    "respond in JSON with 'skills' and 'projects'\"\"\"\n",
    "structured_llm = model_groq.with_structured_output(details,method=\"json_mode\")\n",
    "result = structured_llm.invoke(groq_prompt.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d2531",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20463bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "details(skills=['analytics', 'qualitative and quantitative data analysis', 'data cleaning', 'EDA', 'identifying patterns and trends', 'programming', 'Python(Pandas, Scikit-Learn)', 'SQL', 'data modeling', 'regression', 'classification', 'clustering', 'data collection', 'web scraping', 'image scraping', 'data visualization', 'Power BI', 'Excel', 'Matplotlib', 'Seaborn', 'deep learning', 'ANN', 'CNN', 'OpenCv', 'Media pipe', 'teamwork', 'problem-solving', 'strong communication', 'attention to detail', 'adept at learning new skills'], projects=['credit card fraud detection', 'fake news detection', 'salary prediction', 'spam classifier', 'plant disease prediction – using CNN', 'fruit quality prediction-using CNN', 'virtual painter -using OpenCV and media pipe'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cab06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
