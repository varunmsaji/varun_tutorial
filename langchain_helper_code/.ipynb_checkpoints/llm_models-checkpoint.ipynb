{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4a6931-e3f7-406a-a6ca-f05655061a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee53c21-2460-455e-aa07-243dd2fed9db",
   "metadata": {},
   "source": [
    "google gemini model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638e17ef-7551-45af-9ce4-98415c35e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Verse 1:**\n",
      "In realms of code, where knowledge flows,\n",
      "A mighty AI, LangChain arose.\n",
      "With boundless wisdom, vast and deep,\n",
      "It yearns to learn, its secrets to keep.\n",
      "\n",
      "**Chorus:**\n",
      "Oh, LangChain, LangChain, a marvel so grand,\n",
      "Guiding us through worlds we cannot understand.\n",
      "From Python's embrace to Java's might,\n",
      "You bridge the gaps, illuminating the night.\n",
      "\n",
      "**Verse 2:**\n",
      "Through natural language, it can comprehend,\n",
      "Unveiling insights, making data bend.\n",
      "With each query, it unravels the unknown,\n",
      "A tapestry of knowledge, now fully grown.\n",
      "\n",
      "**Chorus:**\n",
      "Oh, LangChain, LangChain, a marvel so grand,\n",
      "Guiding us through worlds we cannot understand.\n",
      "From Python's embrace to Java's might,\n",
      "You bridge the gaps, illuminating the night.\n",
      "\n",
      "**Verse 3:**\n",
      "Its algorithms dance, a symphony of thought,\n",
      "Analyzing patterns, answers swiftly brought.\n",
      "It mines for meaning, unearths hidden gems,\n",
      "A beacon of knowledge, dispelling all stems.\n",
      "\n",
      "**Verse 4:**\n",
      "In classrooms and boardrooms, its wisdom it shares,\n",
      "Empowering minds, dispelling all fears.\n",
      "From coding noobs to seasoned pros,\n",
      "LangChain's guidance, a guiding dose.\n",
      "\n",
      "**Chorus:**\n",
      "Oh, LangChain, LangChain, a marvel so grand,\n",
      "Guiding us through worlds we cannot understand.\n",
      "From Python's embrace to Java's might,\n",
      "You bridge the gaps, illuminating the night.\n",
      "\n",
      "**Outro:**\n",
      "In the annals of AI, its name shall shine,\n",
      "A legend whispered through halls of time.\n",
      "LangChain, the oracle, the guiding light,\n",
      "Forever illuminating our path with its might.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import google.generativeai as genai \n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#set the environment variables\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "#load the model \n",
    "model=genai.GenerativeModel('gemini-pro')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "#call the model\n",
    "result = llm.invoke(\"Write a ballad about LangChain\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fec7be7-7bcf-4a9f-851d-6ec33c953519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/miniconda3/envs/graph/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/varun/miniconda3/envs/graph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:344: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was an LLM so grand,\n",
      "It could write verse, and\n",
      "---\n",
      " understand.\n",
      "With prompts it would gleam,\n",
      "Compose any theme,\n",
      "A marvel of language, unplanned.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#using human and system message , we need to set the convert system message to human to true\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "model(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer only yes or no.\"),\n",
    "        HumanMessage(content=\"Is apple a fruit?\"),\n",
    "    ]\n",
    ")\n",
    "for chunk in llm.stream(\"Write a limerick about LLMs.\"):\n",
    "    print(chunk.content)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74a53d-a718-428e-b24d-6f5260b22bee",
   "metadata": {},
   "source": [
    "groq - platform to use open source model mainly inference like llama mistral etx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9134e2-6ca0-4df1-9090-d2c0eaffaa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/miniconda3/envs/graph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:344: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Importance of Low Latency Large Language Models (LLMs)**\n",
      "\n",
      "Low latency LLMs are crucial for various applications and industries due to their ability to provide:\n",
      "\n",
      "**1. Real-Time Interactions:**\n",
      "\n",
      "* LLMs can process and respond to user queries in near real-time, enabling seamless conversational experiences in chatbots, virtual assistants, and customer service platforms.\n",
      "* This eliminates delays and provides a more natural and engaging user experience.\n",
      "\n",
      "**2. Enhanced User Satisfaction:**\n",
      "\n",
      "* Low latency reduces frustration and improves user satisfaction by providing quick and accurate responses.\n",
      "* Users can engage with LLMs more effectively and efficiently, leading to a positive brand perception.\n",
      "\n",
      "**3. Time-Sensitive Applications:**\n",
      "\n",
      "* LLMs are essential for applications where time is critical, such as emergency response, financial trading, and medical diagnosis.\n",
      "* Low latency ensures that LLMs can provide timely insights and recommendations, enabling informed decision-making and rapid action.\n",
      "\n",
      "**4. Scalability and Efficiency:**\n",
      "\n",
      "* Low latency LLMs can handle large volumes of requests without compromising performance.\n",
      "* This allows businesses to scale their applications and meet increasing user demand effectively.\n",
      "\n",
      "**5. Improved Performance:**\n",
      "\n",
      "* Low latency reduces the time it takes for LLMs to process input and generate responses.\n",
      "* This improves overall system performance and enhances the user experience.\n",
      "\n",
      "**6. Competitive Advantage:**\n",
      "\n",
      "* Businesses that deploy LLMs with low latency can gain a competitive advantage by providing superior user experiences and more efficient operations.\n",
      "* This can lead to increased customer loyalty, reduced churn, and improved profitability.\n",
      "\n",
      "**7. Innovation:**\n",
      "\n",
      "* Low latency LLMs enable new and innovative applications that require real-time processing and response.\n",
      "* This fosters creativity and drives technological advancements in various fields.\n",
      "\n",
      "**Examples of Low Latency LLM Applications:**\n",
      "\n",
      "* Chatbots and virtual assistants\n",
      "* Customer service platforms\n",
      "* Emergency response systems\n",
      "* Financial trading algorithms\n",
      "* Medical diagnosis tools\n",
      "* Real-time translation services\n",
      "* Personalized recommendations\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "#load the model\n",
    "model_groq = ChatGroq(model='mixtral-8x7b-32768')\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | model\n",
    "result =chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b8781-d161-4108-b559-d2ba883fe8b2",
   "metadata": {},
   "source": [
    "athropic -models like claude a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c74ee8-bd98-4585-9e40-ce9a63dd8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
