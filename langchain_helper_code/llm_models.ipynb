{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4a6931-e3f7-406a-a6ca-f05655061a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee53c21-2460-455e-aa07-243dd2fed9db",
   "metadata": {},
   "source": [
    "google gemini model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638e17ef-7551-45af-9ce4-98415c35e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In realms of code, a tale to unfold,\n",
      "Where LangChain weaves its story, brave and bold.\n",
      "A bard's quill, a language grand,\n",
      "Unveiling secrets at your command.\n",
      "\n",
      "In halls of learning, where knowledge resides,\n",
      "LangChain emerges, a beacon to guide.\n",
      "With syntax sweet and semantics clear,\n",
      "It paints a picture, casting away fear.\n",
      "\n",
      "From simple tasks to quests profound,\n",
      "LangChain's prowess knows no bound.\n",
      "Its algorithms, a symphony of might,\n",
      "Unraveling riddles, bringing forth the light.\n",
      "\n",
      "Like a knight errant, bold and true,\n",
      "LangChain vanquishes errors, through and through.\n",
      "With every line, a story it weaves,\n",
      "Guiding developers, fulfilling their needs.\n",
      "\n",
      "In communities where knowledge flows,\n",
      "LangChain's wisdom, freely it bestows.\n",
      "Coders gather, sharing their lore,\n",
      "Expanding the realm, forevermore.\n",
      "\n",
      "So raise a goblet, let the code resound,\n",
      "To LangChain, the language that knows no bound.\n",
      "May its verses inspire, its power ignite,\n",
      "In the realm of programming, shining ever bright.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import google.generativeai as genai \n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#set the environment variables\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "#load the model \n",
    "model=genai.GenerativeModel('gemini-pro')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "#call the model\n",
    "result = llm.invoke(\"Write a ballad about LangChain\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a740bc",
   "metadata": {},
   "source": [
    "gemini vision pro model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a415ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from IPython.display import Image\n",
    "\n",
    "\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "# # example\n",
    "# message = HumanMessage(\n",
    "#     content=[\n",
    "#         {\n",
    "#             \"type\": \"text\",\n",
    "#             \"text\": \"What's in this image?\",\n",
    "#         },  # You can optionally provide text parts\n",
    "#         {\"type\": \"image_url\", \"image_url\":  \"download (1).png\" },\n",
    "#     ]\n",
    "# )\n",
    "# llm.invoke([message])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec86b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fec7be7-7bcf-4a9f-851d-6ec33c953519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/miniconda3/envs/graph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:344: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was an LLM so grand,\n",
      "It could write poems, at\n",
      "---\n",
      " your command.\n",
      "It could summarize,\n",
      "Translate, and surmise,\n",
      "An AI marvel, truly unplanned.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#using human and system message , we need to set the convert system message to human to true\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "model(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer only yes or no.\"),\n",
    "        HumanMessage(content=\"Is apple a fruit?\"),\n",
    "    ]\n",
    ")\n",
    "for chunk in llm.stream(\"Write a limerick about LLMs.\"):\n",
    "    print(chunk.content)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74a53d-a718-428e-b24d-6f5260b22bee",
   "metadata": {},
   "source": [
    "groq - platform to use open source model mainly inference like llama mistral etx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9134e2-6ca0-4df1-9090-d2c0eaffaa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/miniconda3/envs/graph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:344: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Importance of Low Latency LLMs**\n",
      "\n",
      "Low latency Large Language Models (LLMs) are crucial for real-time applications and interactive user experiences. Latency refers to the time it takes for an LLM to process an input and generate a response. Low latency is essential for:\n",
      "\n",
      "**1. Real-Time Interactions:**\n",
      "\n",
      "* **Chatbots and virtual assistants:** Providing instant responses to user queries, enabling seamless and natural conversations.\n",
      "* **Live transcription and translation:** Transcribing speech and translating it into different languages in real time.\n",
      "\n",
      "**2. Gaming and Virtual Reality:**\n",
      "\n",
      "* **Interactive game experiences:** Generating content and responding to player actions within milliseconds, creating immersive and engaging gameplay.\n",
      "* **Virtual reality environments:** Creating realistic and responsive virtual worlds that update and adapt in real time.\n",
      "\n",
      "**3. Predictive Analytics and Decision-Making:**\n",
      "\n",
      "* **Fraud detection:** Identifying suspicious transactions in real time and preventing financial losses.\n",
      "* **Predictive maintenance:** Monitoring equipment and predicting failures before they occur, enabling proactive maintenance and reducing downtime.\n",
      "\n",
      "**4. Enhanced User Experience:**\n",
      "\n",
      "* **Faster response times:** Improving the overall user experience by providing information and assistance without delays.\n",
      "* **Increased engagement:** Low latency LLMs make interactions more responsive and engaging, leading to increased user satisfaction.\n",
      "\n",
      "**5. Competitive Advantage:**\n",
      "\n",
      "* **Real-time customer service:** Providing instant support to customers, differentiating businesses from competitors with slow response times.\n",
      "* **Innovation in new applications:** Enabling the development of novel applications that require real-time language processing, such as voice assistants and automated translations.\n",
      "\n",
      "**Benefits of Low Latency LLMs:**\n",
      "\n",
      "* **Improved responsiveness**\n",
      "* **Enhanced user satisfaction**\n",
      "* **Increased efficiency and productivity**\n",
      "* **Competitive advantage**\n",
      "* **Innovation in new applications**\n",
      "\n",
      "By minimizing latency, LLMs can unlock the full potential of real-time language processing, empowering businesses and individuals to create more immersive, responsive, and transformative experiences.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "#load the model\n",
    "model_groq = ChatGroq(model='mixtral-8x7b-32768')\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | model\n",
    "result =chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b8781-d161-4108-b559-d2ba883fe8b2",
   "metadata": {},
   "source": [
    "athropic -models like claude a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c74ee8-bd98-4585-9e40-ce9a63dd8360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', response_metadata={'id': 'msg_018q4ao1MAxHnkstLc6Qtt2g', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 23, 'output_tokens': 11}}, id='run-3b47fad7-2005-48b5-90d4-57e1bb709028-0', usage_metadata={'input_tokens': 23, 'output_tokens': 11, 'total_tokens': 34})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a154b98-1b07-4c8c-961d-548d7c9d7e57",
   "metadata": {},
   "source": [
    "open ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab1e053-9f40-4ac4-b524-3549b919880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='എന്റെ പേര് അസിസ്റ്റന്റ് ആണ്.', response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 28, 'total_tokens': 69}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3029d43-0411-41c3-bf7c-b45455eabeb6-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] =os.getenv('OPENAI_NEW')\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model =ChatOpenAI(temperature=0.2)\n",
    "\n",
    "chain = prompt|model\n",
    "\n",
    "chain.invoke(\n",
    "    {'input_language':'english',\n",
    "    'output_language':'malayalam',\n",
    "    'input':'what is your name'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51c695-1b22-4455-b0a6-3c600e18f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "674cadec-9a68-4af3-8b33-95805b0bb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"Low latency Large Language Models (LLMs) are crucial for real-time applications and interactive user experiences. Latency refers to the time it takes for an LLM to process an input and generate a response. Low latency is essential for:\n",
    "\n",
    "**1. Real-Time Interactions:**\n",
    "\n",
    "* **Chatbots and virtual assistants:** Providing instant responses to user queries, enabling seamless and natural conversations.\n",
    "* **Live transcription and translation:** Transcribing speech and translating it into different languages in real time.\n",
    "\n",
    "**2. Gaming and Virtual Reality:**\n",
    "\n",
    "* **Interactive game experiences:** Generating content and responding to player actions within milliseconds, creating immersive and engaging gameplay.\n",
    "* **Virtual reality environments:** Creating realistic and responsive virtual worlds that update and adapt in real time.\n",
    "\n",
    "**3. Predictive Analytics and Decision-Making:**\n",
    "\n",
    "* **Fraud detection:** Identifying suspicious transactions in real time and preventing financial losses.\n",
    "* **Predictive maintenance:** Monitoring equipment and predicting failures before they occur, enabling proactive maintenance and reducing downtime.\n",
    "\n",
    "**4. Enhanced User Experience:**\n",
    "\n",
    "* **Faster response times:** Improving the overall user experience by providing information and assistance without delays.\n",
    "* **Increased engagement:** Low latency LLMs make interactions more responsive and engaging, leading to increased user satisfaction.\n",
    "\n",
    "**5. Competitive Advantage:**\n",
    "\n",
    "* **Real-time customer service:** Providing instant support to customers, differentiating businesses from competitors with slow response times.\n",
    "* **Innovation in new applications:** Enabling the development of novel applications that require real-time language processing, such as voice assistants and automated translations.\n",
    "\n",
    "**Benefits of Low Latency LLMs:**\n",
    "\n",
    "* **Improved responsiveness**\n",
    "* **Enhanced user satisfaction**\n",
    "* **Increased efficiency and productivity**\n",
    "* **Competitive advantage**\n",
    "* **Innovation in new applications**\n",
    "\n",
    "By minimizing latency, LLMs can unlock the full potential of real-time language processing, empowering businesses and individuals to create more immersive, responsive, and transformative experiences.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baea784-f7bc-471a-976a-706235b54ebf",
   "metadata": {},
   "source": [
    "configurable fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda0bf0c-60d3-4ecc-8522-6a613fd947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using configurables\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template2= \"\"\"you are an expert in parsing a text information {text} and generating a list of questions based on the information \"\"\"\n",
    "prompt_template =PromptTemplate.from_template(template2)\n",
    "\n",
    "\n",
    "llm= ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ").configurable_alternatives(\n",
    " ConfigurableField(id=\"llm\"),\n",
    " default_key=\"anthropic\",\n",
    " openai=ChatOpenAI(),\n",
    " gemini=ChatGoogleGenerativeAI(model='gemini-pro'),\n",
    " mistral = ChatGroq(model='mixtral-8x7b-32768') \n",
    ")\n",
    "chain =prompt_template|llm|StrOutputParser()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77047bdb-510f-487d-8210-18b94fb82d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = chain.with_config(configurable={'llm':'gemini'}).invoke({'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e2263d-7a86-4e82-b94c-5795ad6102f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Questions:**\\n\\n1. What is the significance of low latency in Large Language Models (LLMs)?\\n2. List the five key areas where low latency LLMs are crucial.\\n3. Explain the importance of low latency for chatbots and virtual assistants.\\n4. How do LLMs with low latency enhance gaming and virtual reality experiences?\\n5. Describe the role of low latency LLMs in predictive analytics and decision-making.\\n6. How does low latency contribute to an improved user experience?\\n7. What competitive advantage do businesses gain by leveraging low latency LLMs?\\n8. List the benefits of low latency LLMs.\\n9. How can low latency LLMs unlock the potential of real-time language processing?\\n10. Provide examples of how low latency LLMs empower individuals and businesses.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25da05a-828c-450b-b981-bf7e01facc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdff841c",
   "metadata": {},
   "source": [
    "Retrieval augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0081f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "#function to read the pdf document\n",
    "def pdf_read(pdf_filename):\n",
    "  text = \"\"\n",
    "  pdf_reader = PdfReader(pdf_filename)  # Use the single filename directly\n",
    "  for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "  return text\n",
    "\n",
    "text = pdf_read(\"2303.11366v4.pdf\")\n",
    "\n",
    "#chunking the text documents to smaller chunks\n",
    "def get_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "chunks = get_chunks(text)  \n",
    "\n",
    "#embedding the chunks\n",
    "embeddings = SpacyEmbeddings(model_name=\"en_core_web_sm\")\n",
    "\n",
    "#create a vector store\n",
    "def vector_store(chunks):\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_db\")\n",
    "    return vector_store\n",
    "vector=vector_store(chunks)    \n",
    "retriever = vector.as_retriever()\n",
    "# docs =retriever.get_relevant_documents('what is reinforcement learning')\n",
    "\n",
    "#create RAG chian \n",
    "template =\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"what is reinforcement learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d2531",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
